task_name: 'Chef-v0'
render: False
normalization_range: [-1, 1]
xml_path: '/gymnasium_envs/robot_env_description/'  # root of the xml file
xml_file_name: 'scene_reach.xml'  # xml file for training in mujoco
basic_skill_name: ['pick', 'release']  # basic skill, fixed
specified_skill_name: ['reach', 'flip', 'pour']  # specified skills for cooking/kitchen
kitchen_tasks_name: ['flip_bottle', 'pour_water', 'pick_cube']
kitchen_tasks_chain: {'flip_bottle':['reach', 'pick', 'reach', 'flip', 'reach', 'release'],
                      'pour_water': ['reach', 'pick', 'reach', 'pour', 'reach', 'release'],
                      'pick_cube': ['reach', 'pick', 'reach', 'release'],

}
# the chain formed by skills, mainly used in robot part for DMPs
demonstration_trajectories_paths: ''  # loading demonstration trajectory for different skills
demonstration_length: 2000  # the length of the demonstration, uniformed
DMPs_weights_num: 50
max_step_one_episode: 6000
ratio_demonstration2action: 3 # the ratio between the length of demonstration and length of the action step echo epoch
# robot config
ee_pos_limitation_low: [-0.3, 0.1, 0.2]
ee_pos_limitation_high: [0.3, 0.7, 0.8]
ee_rot_limitation_low: [-90, -120, -20]
ee_rot_limitation_high: [0, 120, 20]

task:
  goal_max_pos: [ 0.1, 0.6, 0.45 ]
  goal_min_pos: [ -0.1, 0.4, 0.35 ]
  done_limit: 0.02

alg:
  name: PPO # the algorithm used
  latent_networks: [256, 128]
  policy: 'MlpPolicy'
  clip_range: [0.19, 0.35] # the clip range for PPO, can be used as min to max in linear schedule
  learning_rate: 0.002
  ent_coef: 0.0016 # the entropy coefficasion for PPO
  n_steps: 2048 # the testing step each epoch
  batch_size: 64
  n_epochs: 10
  total_timesteps: 6000000
  checkpoint_freq: 200000
  eval_freq: 20000
  log_path: '/home/yi/project_ghent/chef/logs/'  # the root of tensorboard logging path
  model_path: '/home/yi/robotic_manipulation/SoftBodyChef/models/'


