task_name: 'Chef-v0'
render: False
normalization_range: [-1, 1]
xml_path: '/gymnasium_envs/robot_env_description/'  # root of the xml file
xml_file_name: 'scene_reach.xml'  # xml file for training in mujoco
basic_skill_name: ['pick', 'release']  # basic skill, fixed
specified_skill_name: ['reach', 'flip', 'pour']  # specified skills for cooking/kitchen
kitchen_tasks_name: ['flip_bottle', 'pour_water', 'pick_cube']
kitchen_tasks_chain: {'flip_bottle':['reach', 'pick', 'reach', 'flip', 'reach', 'release'],
                      'pour_water': ['reach', 'pick', 'reach', 'pour', 'reach', 'release'],
                      'pick_cube': ['reach', 'pick', 'reach', 'release'],

}
# the chain formed by skills, mainly used in robot part for DMPs
demonstration_trajectories_paths: ''  # loading demonstration trajectory for different skills
demonstration_length: 2000  # the length of the demonstration, uniformed
DMPs_weights_num: 60
max_step_one_episode: 6000

robot:
  # robot config
  ee_pos_limitation_low: [ -0.3, 0.1, 0.2 ]
  ee_pos_limitation_high: [ 0.3, 0.7, 0.8 ]
  ee_rot_limitation_low: [ -135, -120, -20 ]
  ee_rot_limitation_high: [ 0, 120, 20 ]
  ee_pos_increment: 0.02  # the increment of the EEF pos when action space is 6-DoF of EEF (meter)
  ee_rot_increment: 5  # same as ee_pos_increment' meaning (degree)

task:
  goal_max_pos: [ 0.1, 0.6, 0.45 ]
  goal_min_pos: [ -0.1, 0.4, 0.35 ]
  done_limit: 0.01  # the done limit in env0 (reach skill, distance, unit: meter)

alg:
  name: PPO # the algorithm used
  latent_networks: [128, 128]
  policy: 'MlpPolicy'
  clip_range: [0.17, 0.04] # the clip range for PPO, can be used as min to max in linear schedule
  learning_rate: 0.00035
  ent_coef: 0.0016 # the entropy coefficasion for PPO
  target_kl: 0.03
  n_steps: 2048 # the testing step each epoch
  batch_size: 64
  n_epochs: 10
  total_timesteps: 8000000
  checkpoint_freq: 200000
  eval_freq: 20000
  log_path: '/home/yi/project_ghent/chef/logs/'  # the root of tensorboard logging path
  model_path: '/home/yi/robotic_manipulation/SoftBodyChef/models/'

reward_shaping:
  weights: [1, 0.01]  # the weight for different parts
  log_base: 0.5  # logarithm configuration (0 < log_base < 1)
  dis_threshold: 0.1  # the threshold for EEF distance, pos + rot
  max_EEF_distance: 2  # for testing reach skill, maximum distance in EEF
  max_traj_diff: 2  # the maximum diff between DMPs traj and current traj



